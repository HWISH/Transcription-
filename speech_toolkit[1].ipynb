{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9207a3f7-dbf1-4550-99b0-40021f71cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"Audio14_306_20240607213835385014.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37a96434-39d5-4baa-af67-662e87edf0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Audio Resampling ############################\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pydub import AudioSegment\n",
    "import scipy.io.wavfile as wavfile\n",
    "\n",
    "def resample_audio(input_path, output_path, target_sample_rate):\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "    if audio.channels > 1:\n",
    "        audio = audio.split_to_mono()[0]    \n",
    "    audio = audio.set_frame_rate(target_sample_rate)    \n",
    "    audio.export(output_path, format=\"wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86c8a04c-ab13-43dc-8016-489b496ae965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 16 files: 100%|███████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 141281.82it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############################ SPEECH ENHANCEMENT ############################\n",
    "import io\n",
    "import soundfile\n",
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "from espnet2.bin.enh_inference import SeparateSpeech\n",
    "\n",
    "d = ModelDownloader()\n",
    "cfg = d.download_and_unpack(\n",
    "    \"espnet/Wangyou_Zhang_chime4_enh_train_enh_conv_tasnet_raw\")\n",
    "separate_speech = {}\n",
    "# For models downloaded from GoogleDrive, you can use the following script:\n",
    "enh_model_sc = SeparateSpeech(\n",
    "    train_config=cfg[\"train_config\"],\n",
    "    model_file=cfg[\"model_file\"],\n",
    "    # for segment-wise process on long speech\n",
    "    normalize_segment_scale=False,\n",
    "    show_progressbar=True,\n",
    "    ref_channel=1,\n",
    "    normalize_output_wav=True,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "def speech_enhancement(file_name):\n",
    "    mixwav_mc, sr = soundfile.read(file_name)\n",
    "    # mixwav.shape: num_samples, num_channels\n",
    "    # mixwav_sc = mixwav_mc[:,4]\n",
    "    mixwav_sc = mixwav_mc  # [:,4]\n",
    "    wave = enh_model_sc(mixwav_sc[None, ...], sr)\n",
    "    soundfile.write(file_name, wave[0].squeeze(), 16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9176643c-3285-4be2-84b1-818fb365ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Voice Activity Detection ############################\n",
    "\n",
    "# !pip install speechbrain\n",
    "from speechbrain.inference.VAD import VAD\n",
    "\n",
    "VAD = VAD.from_hparams(source=\"speechbrain/vad-crdnn-libriparty\", savedir=\"pretrained_models/vad-crdnn-libriparty\")\n",
    "\n",
    "def speechbrain_vad(file_name):\n",
    "    boundaries = VAD.get_speech_segments(file_name)\n",
    "    VAD.save_boundaries(boundaries)\n",
    "    VAD.save_boundaries(boundaries, save_path=file_name.replace(\".wav\", \".txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e20006-1782-4b2f-983b-303c31790bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb6765c9-d526-44c6-b375-a1101da8538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_audio(file_name, file_name.replace(\".mp3\", \".wav\"), 16000)\n",
    "speech_enhancement(file_name.replace(\".mp3\", \".wav\"))\n",
    "speechbrain_vad(file_name.replace(\".mp3\", \".wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c331a88-4960-4641-97c4-0ab46b0a6bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' raz fatar nasi, wapa fatar na btsera, ni tuka txanson, pakafia nou, sembula weyo,',\n",
       " 'chunks': [{'timestamp': (0.0, 2.66),\n",
       "   'text': ' raz fatar nasi, wapa fatar na btsera,'},\n",
       "  {'timestamp': (2.74, 4.56), 'text': ' ni tuka txanson, pakafia nou,'},\n",
       "  {'timestamp': (4.74, 5.56), 'text': ' sembula weyo,'}]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################ Speech-To-Text ############################\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\" # or whisper-small, whisper-tiny, etc.\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "def stt_whisper(sample, language):\n",
    "    try:\n",
    "        result = pipe(sample, generate_kwargs={\"language\": language, \"task\": \"transcribe\"})\n",
    "    except:\n",
    "        result = {\"text\" : \"\", \"chunks\" : []}\n",
    "    return result\n",
    "\n",
    "text = stt_whisper(file_name, \"malagasy\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f4598-6a58-4402-8119-58495a915cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
